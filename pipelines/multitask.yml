version: 1
description: ""
parallel: 1
properties:
  auto_dependency: false
  pipeline_type: "databricks_multitask"
jobs:
- processing_test:
    job_parameters:
      param1: "value1"
      param2: "value2"
    spark_properties:
      job_compute_key: "compute_1"
- csv_to_json_copy2:
    job_parameters:
      param3: "value3"
    spark_properties:
      job_compute_key: "compute_2"
- csv_to_json_copy1:
    job_parameters:
      param4: "value4"
    spark_properties:
      cluster_id: "existing-cluster-id"
- column_mapping_test:
    spark_properties:
      driver_node_type_id: "Standard_DS4_v2"
      node_type_id: "Standard_DS4_v2"
- "local_csv_to_json"
computes:
- name: "compute_1"
  properties:
    spark_version: "9.1.x-scala2.12"
    driver_node_type_id: "Standard_DS3_v2"
    node_type_id: "Standard_DS3_v2"
    workers:
      num_workers: "1"
    spark_conf:
      conf1: "vaule1"
      conf2: "value2"
    spark_env_vars:
      env2: "value2"
      evn1: "value1"
    init_scripts:
    - "script1"
    cluster_tags:
      label1: "value1"
    retry_properties:
      max_retries: 2
      min_retry_interval_millis: 300000
- name: "compute_2"
  properties:
    spark_version: "9.1.x-scala2.12"
    driver_node_type_id: "Standard_DS3_v2"
    node_type_id: "Standard_DS3_v2"
    workers:
      autoscale:
        min_workers: "1"
        max_workers: "2"
